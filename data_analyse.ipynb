{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Clustering of CelebA Images with PyTorch and Plotly\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- Use **PyTorch** to extract features from the CelebA dataset using a pre-trained VGG16 model.\n",
    "- Perform dimensionality reduction using **PCA**.\n",
    "- Apply **K-Means** clustering to group similar images.\n",
    "- Use **t-SNE** for visualization.\n",
    "- Create an interactive plot within the Jupyter Notebook where clicking on a point displays the corresponding image."
   ],
   "id": "3641ce0b1abf2495"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure you have the following libraries installed:\n",
    "\n",
    "```bash\n",
    "pip install numpy pandas tqdm pillow torch torchvision scikit-learn plotly ipywidgets\n",
    "```"
   ],
   "id": "463c438a3ea53f18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries\n",
    "\n",
    "First, we import all the necessary libraries.\n"
   ],
   "id": "cbff38ac035bebce"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:47:40.409049Z",
     "start_time": "2024-10-13T20:47:40.401042Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Dimensionality reduction and clustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Widgets for interactive display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ],
   "id": "d99f4d27e4caeb16",
   "outputs": [],
   "execution_count": 134
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check for GPU Availability\n",
    "\n",
    "We check if a GPU is available and set the device accordingly. "
   ],
   "id": "8553a9715c31c285"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:47:40.502300Z",
     "start_time": "2024-10-13T20:47:40.497637Z"
    }
   },
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "id": "f84d52a422f2d8ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up Image Path and Variables\n",
    "\n",
    "We set the path to the CelebA dataset and list the image files. "
   ],
   "id": "2586db42ca04f753"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:50:00.141288Z",
     "start_time": "2024-10-13T20:50:00.099943Z"
    }
   },
   "source": [
    "# Set the path to the CelebA or Flowers dataset directory\n",
    "current_dir = os.getcwd()\n",
    "#image_folder = os.path.join(current_dir, 'datasets', 'celeba', 'img_align_celeba')\n",
    "image_folder = os.path.join(current_dir, 'datasets', 'flowers-102', 'jpg')\n",
    "\n",
    "print(f\"Image folder path: {image_folder}\")\n",
    "\n",
    "# List image files\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
    "\n",
    "# Limit the number of images to manage memory usage\n",
    "max_images = 8000  # Set to desired limit\n",
    "image_files = image_files[:max_images]\n",
    "\n",
    "# Number of n_clusters=20\n",
    "clusters=20\n",
    "\n",
    "# Display the number of images\n",
    "print(f\"Number of images: {len(image_files)}\")"
   ],
   "id": "48789d5eeed3ce70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image folder path: /home/user/CtrlAltDiffuse/datasets/flowers-102/jpg\n",
      "Number of images: 8000\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Note**: Ensure that the `image_folder` path points to the correct location of your CelebA or Flowers dataset. ",
   "id": "c882cca8acdbfb0d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Image Transformations\n",
    "\n",
    "We define the image transformations to preprocess the images. Since not all images are size 256x256, we resize them."
   ],
   "id": "77f538c5d66fae97"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:50:05.616591Z",
     "start_time": "2024-10-13T20:50:05.608538Z"
    }
   },
   "source": [
    "# Define image transformations (no resizing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "911d9c94f3e5aabd",
   "outputs": [],
   "execution_count": 149
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Pre-trained VGG16 Model\n",
    "\n",
    "We load the pre-trained VGG16 model from PyTorch, setting it to evaluation mode. "
   ],
   "id": "a0bdbfd93c873f8c"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:50:12.455582Z",
     "start_time": "2024-10-13T20:50:09.468217Z"
    }
   },
   "source": [
    "# Load the pre-trained VGG16 model\n",
    "model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "print(\"Model loaded.\")"
   ],
   "id": "9044cbcdd18a7078",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Feature Extractor\n",
    "\n",
    "We create a feature extractor by removing the classification layers from the VGG16 model. "
   ],
   "id": "b474815e3fbaa026"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:50:13.679183Z",
     "start_time": "2024-10-13T20:50:13.672265Z"
    }
   },
   "source": [
    "# Create a feature extractor by removing the classification layers\n",
    "class FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # Use all layers except the last classifier layers\n",
    "        self.features = model.features\n",
    "        # Add adaptive pooling to ensure consistent output size\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "feature_extractor = FeatureExtractor(model).to(device)"
   ],
   "id": "adb6aec6a3d0c1f2",
   "outputs": [],
   "execution_count": 151
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Features from Images\n",
    "\n",
    "We loop through the images, preprocess them, and extract features using the feature extractor. "
   ],
   "id": "5d95082c9076e9f2"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:52:01.135558Z",
     "start_time": "2024-10-13T20:50:16.868845Z"
    }
   },
   "source": [
    "# Extract features from images\n",
    "print(\"Extracting features...\")\n",
    "features_list = []\n",
    "\n",
    "for image_file in tqdm(image_files):\n",
    "    img_path = os.path.join(image_folder, image_file)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature = feature_extractor(image)\n",
    "    feature = feature.cpu().numpy().flatten()\n",
    "    features_list.append(feature)\n",
    "\n",
    "features_array = np.array(features_list)\n",
    "print(f\"Shape of extracted features: {features_array.shape}\")"
   ],
   "id": "19dfbcf09ba9a7dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [01:43<00:00, 77.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of extracted features: (8000, 25088)\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Note**: This step may take some time depending on the number of images and your hardware. ",
   "id": "16a5327870e252f1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Apply PCA for Dimensionality Reduction\n",
    "\n",
    "We reduce the dimensionality of the features using PCA. "
   ],
   "id": "716ab5265f8a5044"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:52:14.546356Z",
     "start_time": "2024-10-13T20:52:01.224690Z"
    }
   },
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=100)\n",
    "features_pca = pca.fit_transform(features_array)\n",
    "print(f\"Shape after PCA: {features_pca.shape}\")"
   ],
   "id": "56776079770b5ede",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after PCA: (8000, 100)\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Perform K-Means Clustering\n",
    " \n",
    "We cluster the images using K-Means clustering."
   ],
   "id": "5eb36cc1a1072e3c"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:52:14.742865Z",
     "start_time": "2024-10-13T20:52:14.631087Z"
    }
   },
   "source": [
    "# Perform K-Means clustering\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(features_pca)\n",
    "print(\"Clustering completed.\")"
   ],
   "id": "82e61bb6ba873490",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering completed.\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Use t-SNE for Visualization\n",
    "\n",
    "We use t-SNE to reduce the data to two dimensions for visualization. "
   ],
   "id": "78e4a7654e800ede"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:52:30.383752Z",
     "start_time": "2024-10-13T20:52:14.818289Z"
    }
   },
   "source": [
    "# Use t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "features_tsne = tsne.fit_transform(features_pca)\n",
    "print(\"t-SNE transformation completed.\")"
   ],
   "id": "5bdec71708485e7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE transformation completed.\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prepare DataFrame for Visualization \n",
    "\n",
    "We create a DataFrame containing the t-SNE results and other relevant data."
   ],
   "id": "6f5d2f1a713f11fb"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:52:30.637275Z",
     "start_time": "2024-10-13T20:52:30.627553Z"
    }
   },
   "source": [
    "# Prepare DataFrame for visualization\n",
    "tsne_df = pd.DataFrame({\n",
    "    'TSNE1': features_tsne[:, 0],\n",
    "    'TSNE2': features_tsne[:, 1],\n",
    "    'Cluster': kmeans_labels.astype(str),\n",
    "    'Image File': image_files\n",
    "})"
   ],
   "id": "14c0a3c46edcb351",
   "outputs": [],
   "execution_count": 156
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Interactive Plot and Image Display\n",
    "\n",
    "We create an interactive plot using Plotly's `FigureWidget` and display the image corresponding to the selected point. "
   ],
   "id": "259076b40ac16e3b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:52:30.739235Z",
     "start_time": "2024-10-13T20:52:30.684636Z"
    }
   },
   "source": [
    "# Create a FigureWidget\n",
    "scatter = go.FigureWidget(\n",
    "    data=go.Scattergl(\n",
    "        x=tsne_df['TSNE1'],\n",
    "        y=tsne_df['TSNE2'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=tsne_df['Cluster'].astype(int),\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            size=5\n",
    "        ),\n",
    "        customdata=tsne_df['Image File'],\n",
    "        hovertemplate='<b>Image File:</b> %{customdata}<br>' +\n",
    "                      '<b>Cluster:</b> %{marker.color}<br>' +\n",
    "                      '<extra></extra>'\n",
    "    )\n",
    ")\n",
    "\n",
    "scatter.update_layout(\n",
    "    title='t-SNE Visualization of CelebA Images',\n",
    "    xaxis_title='t-SNE Dimension 1',\n",
    "    yaxis_title='t-SNE Dimension 2',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Create an Image widget to display the selected image\n",
    "image_widget = widgets.Image(\n",
    "    format='jpg',\n",
    "    width=256,\n",
    "    height=256\n",
    ")\n",
    "\n",
    "# Create a function to handle click events\n",
    "def update_image(trace, points, selector):\n",
    "    if points.point_inds:\n",
    "        ind = points.point_inds[0]\n",
    "        image_file = tsne_df.iloc[ind]['Image File']\n",
    "        img_path = os.path.join(image_folder, image_file)\n",
    "        with open(img_path, 'rb') as f:\n",
    "            img_data = f.read()\n",
    "            image_widget.value = img_data\n",
    "\n",
    "# Attach the click event handler\n",
    "scatter.data[0].on_click(update_image)\n",
    "\n",
    "# Display the plot and the image widget\n",
    "container = widgets.VBox([scatter, image_widget])\n",
    "display(container)"
   ],
   "id": "1a126fcf928525bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(FigureWidget({\n",
       "    'data': [{'customdata': array(['image_00666.jpg', 'image_07908.jpg', 'image_…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa208ee29dfb4679b8ff0b4f199f8c37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saved CelebA Plot",
   "id": "df4bc25b6fe4c654"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:49:12.506663Z",
     "start_time": "2024-10-13T20:49:12.493957Z"
    }
   },
   "cell_type": "code",
   "source": "display(container)",
   "id": "c4547caa47490dc0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(FigureWidget({\n",
       "    'data': [{'customdata': array(['045151.jpg', '005924.jpg', '058771.jpg', ...…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2141a60109f142cc98bfca4c41d6a656"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saved Flowers Plot\n",
   "id": "807ed8c21f87d171"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T20:52:44.876701Z",
     "start_time": "2024-10-13T20:52:44.865848Z"
    }
   },
   "cell_type": "code",
   "source": "display(container)\n",
   "id": "5f3883fd6faf205b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(FigureWidget({\n",
       "    'data': [{'customdata': array(['image_00666.jpg', 'image_07908.jpg', 'image_…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa208ee29dfb4679b8ff0b4f199f8c37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 158
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
